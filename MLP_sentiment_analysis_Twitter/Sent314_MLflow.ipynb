{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d279705-65f1-44bc-9cdb-04d3ab641dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import time\n",
    "import pathlib\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.backends.cudnn as cudnn\n",
    "\n",
    "from distoptim import fedavg\n",
    "import util_text as util\n",
    "import models\n",
    "from params import args_parser\n",
    "\n",
    "# define device\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\" # GPU does not speed up training here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72fd96d-b798-44aa-8280-9d800ba1c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peng/workspace/Power-of-Choice/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/peng/workspace/Power-of-Choice/venv/lib/python3.10/site-packages/pydantic/_internal/_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Repository initialized!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/363474a791ea4ebf8b0e375509b6c86a', creation_time=1700855979430, experiment_id='4', last_update_time=1700855979430, lifecycle_stage='active', name='Twitter_Sentiment_Analysis', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "from mlflow.models import infer_signature\n",
    "dagshub.init(repo_owner='peng-ju', repo_name='Power-of-Choice', mlflow=True)\n",
    "mlflow.set_tracking_uri=\"https://dagshub.com/peng-ju/Power-of-Choice.mlflow\"\n",
    "mlflow.set_experiment(experiment_name=\"Twitter_Sentiment_Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0cce8e-ea26-4fff-b5db-074c2a124e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_parser():\n",
    "    def __init__(self):\n",
    "        self.name = \"default\"\n",
    "        self.model = \"MLP\"\n",
    "        self.alpha = 0.2\n",
    "        self.num_classes = 1\n",
    "        self.lr = 0.005\n",
    "        self.momentum = 0\n",
    "        self.bs = 32\n",
    "        self.rounds = 1\n",
    "        self.localE = 100\n",
    "        self.decay = 1 # 1: decay LR, 0: no decay\n",
    "        self.size = 8 # 8, selected clients for updating the model: m\n",
    "        self.powd = 32\n",
    "        self.fracC = 0.2\n",
    "        self.seltype = \"rand\"\n",
    "        self.ensize = 314\n",
    "        self.rank = 0\n",
    "        self.rnd_ratio = 0.1\n",
    "        self.delete_ratio = 0.75\n",
    "        self.seed = 1\n",
    "        self.commE = \"store_true\"\n",
    "        self.constant = \"store_true\"\n",
    "        self.dataset = \"twitter\"\n",
    "\n",
    "args = Args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b71d44-6174-489e-a158-1b6453e00fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(format='%(levelname)s - %(message)s', level=logging.INFO)\n",
    "# logging.debug('This message should appear on the console')\n",
    "\n",
    "# def run(rank, size):\n",
    "#     print(\"run start \\n\")\n",
    "#     # initiate experiments folder\n",
    "#     save_path = './logs/'\n",
    "#     fold = 'lr{:.4f}_bs{}_cp{}_a{:.2f}_e{}_r0_n{}_f{:.2f}/'.format(args.lr, \n",
    "#                                                                    args.bs, \n",
    "#                                                                    args.localE, \n",
    "#                                                                    args.alpha, \n",
    "#                                                                    args.seed,\n",
    "#                                                                    args.ensize, \n",
    "#                                                                    args.fracC)\n",
    "#     if args.commE:\n",
    "#         fold = 'com_'+fold\n",
    "#     folder_name = save_path+args.name+'/'+fold\n",
    "#     file_name = '{}_rr{:.2f}_dr{:.2f}_lr{:.3f}_bs{:d}_cp{:d}_a{:.2f}_e{}_r{}_n{}_f{:.2f}_p{}.csv'.format(args.seltype,\n",
    "#                                                                                                          args.rnd_ratio, \n",
    "#                                                                                                          args.delete_ratio, \n",
    "#                                                                                                          args.lr, \n",
    "#                                                                                                          args.bs, \n",
    "#                                                                                                          args.localE,\n",
    "#                                                                                                          args.alpha, \n",
    "#                                                                                                          args.seed, \n",
    "#                                                                                                          rank, \n",
    "#                                                                                                          args.ensize, \n",
    "#                                                                                                          args.fracC, \n",
    "#                                                                                                          args.powd)\n",
    "                                                    \n",
    "#     pathlib.Path(folder_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # initiate log files\n",
    "#     saveFileName = folder_name + file_name\n",
    "#     args.out_fname = saveFileName\n",
    "#     with open(args.out_fname, 'w+') as f:\n",
    "#         print('Epoch,itr,loss,trainloss,avg:Loss,Prec@1,avg:Prec@1,val,trainval,updtime,comptime,seltime,entime,testacc,testloss', file=f)\n",
    "\n",
    "#     # seed for reproducibility\n",
    "#     random.seed(args.seed)\n",
    "#     np.random.seed(args.seed)\n",
    "#     torch.manual_seed(args.seed)\n",
    "#     torch.cuda.manual_seed(args.seed)\n",
    "#     # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#     with mlflow.start_run(run_name=f\"Sentiment_Analysis_seltype_{args.seltype}_powd_{args.powd}_num_users_{args.ensize}_rounds_{args.rounds}\") as run:\n",
    "#         # MLflow\n",
    "#         params = {}\n",
    "#         params[\"Learning Rate\"] = args.lr\n",
    "#         params[\"Batch Size\"] = args.bs\n",
    "#         params[\"Total Client Number\"] = args.ensize\n",
    "#         params[\"Selected client number: m\"] = args.size\n",
    "#         params[\"Local iteration number\"] = args.localE\n",
    "#         params[\"Powd\"] = args.powd\n",
    "\n",
    "#         # load data\n",
    "#         partition, train_loader, test_loader, dataratios, traindata = util.partition_dataset(size, args, 0)\n",
    "#         print(\"\\n dataratios: \", dataratios)\n",
    "\n",
    "#         # tracking client loss values, frequency for each client\n",
    "#         # args.ensize -- number of clients\n",
    "#         client_freq, client_loss_proxy = np.zeros(args.ensize), np.zeros(args.ensize)\n",
    "\n",
    "#         # initialization for client selection\n",
    "#         cli_loss, cli_freq, cli_val = np.zeros(args.ensize)+1, np.zeros(args.ensize), np.zeros(args.ensize)\n",
    "\n",
    "#         # select client for the 1st round\n",
    "#         replace_param = False\n",
    "#         if args.seltype =='rand':\n",
    "#             replace_param = True\n",
    "\n",
    "#         # user id being selected\n",
    "#         sel_idx = np.random.choice(args.ensize, size=args.size, replace=replace_param)\n",
    "\n",
    "#         # define multilayer perceptron neural network model for sentiment analysis\n",
    "#         model = models.MLP_text(input_size=200, dim_hidden1=128, dim_hidden2=86, dim_hidden3=30, dim_out=1).to(device)\n",
    "        \n",
    "#         # allocate buffer for global and aggregate parameters\n",
    "#         # ref: https://discuss.pytorch.org/t/how-to-assign-an-arbitrary-tensor-to-models-parameter/44082/3\n",
    "#         global_parameters = []\n",
    "#         aggregate_parameters = []\n",
    "#         with torch.no_grad():\n",
    "#             for param in model.parameters():\n",
    "#                 global_parameters.append(param.detach().clone())\n",
    "#                 aggregate_parameters.append(torch.zeros_like(param)) \n",
    "        \n",
    "#         # criterion\n",
    "#         # criterion = nn.NLLLoss().to(device)  # for multi-class classifier.\n",
    "#         criterion =nn.BCELoss().to(device)  # for binary classifier\n",
    "#         params[\"Criterion\"] = \"nn.BCELoss()\"\n",
    "\n",
    "#         # select optimizer according to algorithm\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                                     lr=args.lr, \n",
    "#                                     momentum=args.momentum, \n",
    "#                                     nesterov=False,\n",
    "#                                     weight_decay=1e-4)\n",
    "#         params[\"Optimizer\"] = \"torch.optim.SGD\"\n",
    "\n",
    "#         test_loss_rnd = []\n",
    "#         test_accu_rnd = []\n",
    "#         train_loss_rnd = []\n",
    "#         train_accu_rnd = []\n",
    "\n",
    "\n",
    "#         # start communication rounds\n",
    "#         for rnd in range(args.rounds):\n",
    "#             round_start = time.time()\n",
    "\n",
    "#             # (optional) decay learning rate according to round index\n",
    "#             if args.decay == True:\n",
    "#                 # update_learning_rate(optimizer, rnd, args.lr)\n",
    "#                 if rnd == 60:\n",
    "#                     lr = args.lr/2\n",
    "#                     logging.info(\"Updating learning rate to {}\".format(lr))\n",
    "#                     for param_group in optimizer.param_groups:\n",
    "#                         param_group[\"lr\"] = lr\n",
    "\n",
    "#                 if rnd == 120:\n",
    "#                     lr = args.lr/4\n",
    "#                     logging.info(\"Updating learning rate to {}\".format(lr))\n",
    "#                     for param_group in optimizer.param_groups:\n",
    "#                         param_group[\"lr\"] = lr\n",
    "\n",
    "#             # zero aggregate parameters for accumulation of local parameters\n",
    "#             with torch.no_grad():\n",
    "#                 for param in aggregate_parameters:\n",
    "#                     param.zero_()\n",
    "\n",
    "#             # for each client `i`\n",
    "#             for i in sel_idx:\n",
    "#                 # send global parameters to client `i`\n",
    "#                 with torch.no_grad():\n",
    "#                     for param, global_param in zip(model.parameters(), global_parameters):\n",
    "#                         param.copy_(global_param)\n",
    "                \n",
    "#                 # run E steps of SGD on client `i`\n",
    "#                 loss_final = 0\n",
    "#                 comm_update_start = time.time()\n",
    "#                 for t in range(args.localE):\n",
    "#                     singlebatch_loader = util.partitiondata_loader(partition, i, args.bs, traindata)\n",
    "#                     loss, model = train_text(rank, model, criterion, optimizer, singlebatch_loader, t)\n",
    "#                     loss_final += loss/args.localE #average over localE iterations\n",
    "#                 comm_update_end = time.time()\n",
    "#                 update_time = comm_update_end - comm_update_start\n",
    "\n",
    "#                 # send local parameters from client `i` to server for aggregation\n",
    "#                 with torch.no_grad():\n",
    "#                     weight = 1/args.size\n",
    "#                     for aggregate_param, param in zip(aggregate_parameters, model.parameters()):\n",
    "#                         aggregate_param.add_(param, alpha=weight)\n",
    "                \n",
    "#                 # update client frequency and loss values\n",
    "#                 client_freq[i] += 1\n",
    "#                 client_loss_proxy[i] = loss_final\n",
    "\n",
    "#             not_visited = np.where(client_freq == 0)[0]\n",
    "#             for j in not_visited:\n",
    "#                 if args.seltype == \"afl\":\n",
    "#                     client_loss_proxy[j] = -np.inf\n",
    "#                 else:\n",
    "#                     client_loss_proxy[j] = np.inf\n",
    "\n",
    "#             # update global parameters\n",
    "#             with torch.no_grad():\n",
    "#                 for global_param, aggregate_param in zip(global_parameters, aggregate_parameters):\n",
    "#                     global_param.copy_(aggregate_param)\n",
    "\n",
    "#             # set model with global parameters\n",
    "#             with torch.no_grad():\n",
    "#                 for param, global_param in zip(model.parameters(), global_parameters):\n",
    "#                     param.copy_(global_param)\n",
    "\n",
    "#             # evaluate test accuracy\n",
    "#             test_acc, test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "#             # evaluate loss values and sync selected frequency\n",
    "#             cli_loss, cli_comptime = evaluate_client(model, criterion, partition, traindata)\n",
    "#             train_loss = sum([cli_loss[i]*dataratios[i] for i in range(args.ensize)])\n",
    "#             # train_loss1 = sum(cli_loss)/args.ensize\n",
    "\n",
    "#             # select clients for the next round\n",
    "#             sel_time, comp_time = 0, 0\n",
    "#             sel_time_start = time.time()\n",
    "#             \"\"\"\n",
    "#             noteL cli_val, rnd are not useful?\n",
    "#             \"\"\"\n",
    "#             sel_idx, rnd_idx = util.sel_client(dataratios, cli_loss, cli_val, args, rnd)\n",
    "#             # print(f\"len rnd_idx {len(rnd_idx)} idxs_users {len(idxs_users)}\")\n",
    "#             sel_time_end = time.time()\n",
    "#             sel_time = sel_time_end - sel_time_start\n",
    "\n",
    "#             if args.seltype == \"pow-d\" or args.seltype == \"pow-dint\":\n",
    "#                 comp_time = max([cli_comptime[int(i)] for i in rnd_idx])\n",
    "\n",
    "#             # record metrics\n",
    "#             round_end = time.time()\n",
    "#             round_duration = round(round_end - round_start, 1)\n",
    "#             logging.info(f\"[{round_duration} s] Round {rnd} rank {rank} test accuracy {test_acc:.3f} test loss {test_loss:.3f} train loss {train_loss:.3f}\")\n",
    "#             # MLflow\n",
    "#             mlflow.log_metric(key=\"train_loss\", value=train_loss, step=rnd)\n",
    "#             # mlflow.log_metric(key=\"train_accurarcy\", value=train_acc, step=rnd)\n",
    "#             mlflow.log_metric(key=\"test_loss\", value=test_loss, step=rnd)\n",
    "#             mlflow.log_metric(key=\"test_accurarcy\", value=test_acc, step=rnd)\n",
    "\n",
    "#             test_loss_rnd.append(test_loss)\n",
    "#             test_accu_rnd.append(test_acc)\n",
    "#             train_loss_rnd.append(train_loss)\n",
    "#             # train_accu_rnd.append(train_acc)\n",
    "            \n",
    "#             # itr = -1 for overal result\n",
    "#             with open(args.out_fname, '+a') as f:\n",
    "#                 print('{ep},{itr},{loss:.4f},{trainloss:.4f},{filler},'\n",
    "#                     '{filler},{filler},'\n",
    "#                     '{val:.4f},{other:.4f},{updtime:.4f},{comptime:.4f},{seltime:.4f},{entime:.4f}, {testacc:.4f}, {testloss:.4f}'\n",
    "#                     .format(ep=rnd, itr=-1, loss=test_loss, trainloss=train_loss,\n",
    "#                             filler=-1, val=test_acc, other=train_loss, updtime=update_time, comptime=comp_time,\n",
    "#                             seltime=sel_time, entime=update_time+comp_time+sel_time, testacc=test_acc, testloss=test_loss), file=f)\n",
    "        \n",
    "#         # MLflow\n",
    "#         params[\"minimum_tweets\"] = \"64\"\n",
    "#         # train_loader = torch.utils.data.DataLoader(traindata, batch_size=args.bs, shuffle=False,\n",
    "#         #                                             pin_memory=True)\n",
    "#         data, target = next(iter(train_loader))\n",
    "#         signature = infer_signature(data, model(data))\n",
    "#         model_info = mlflow.pyfunc.log_model(python_model=model, \n",
    "#                                              artifact_path=\"my_model\", \n",
    "#                                              signature=signature\n",
    "#                                             )\n",
    "        \n",
    "#         mlflow.log_params(params)\n",
    "\n",
    "\n",
    "# def evaluate_client(model, criterion, partition, traindata):\n",
    "\n",
    "#     '''\n",
    "#     Evaluating each client's local loss values for the current global model for client selection\n",
    "#     :param model: current global model\n",
    "#     :param criterion: loss function\n",
    "#     :param partition: dataset dict for clients\n",
    "#     :return: cli_loss = list of local loss values, cli_comptime = list of computation time\n",
    "#     '''\n",
    "\n",
    "#     cli_comptime, cli_loss = [], []\n",
    "#     model.eval()\n",
    "\n",
    "#     # Get data from client to evaluate local loss on\n",
    "#     for i in range(args.ensize):\n",
    "#         partitioned = partition[i]\n",
    "\n",
    "#         # cpow-d\n",
    "#         if args.commE:\n",
    "#             seldata_idx = random.sample(range(len(partitioned)), k=int(min(args.bs, len(partitioned))))\n",
    "#         else:\n",
    "#             seldata_idx = partitioned\n",
    "\n",
    "#         other = torch.utils.data.Subset(traindata, indices=seldata_idx)\n",
    "#         train_loader = torch.utils.data.DataLoader(other, batch_size=args.bs, shuffle=False,\n",
    "#                                                     pin_memory=True)\n",
    "        \n",
    "#         # Compute local loss values or proxies for the clients\n",
    "#         tmp, total = 0,0\n",
    "#         with torch.no_grad():\n",
    "#             comptime_start = time.time()\n",
    "#             for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#                 data = data.to(device,non_blocking=True)\n",
    "#                 target = target.to(device,non_blocking=True)\n",
    "#                 vec_target = vector_encoding(target)\n",
    "\n",
    "#                 vec_target = vec_target.to(device,non_blocking=True)\n",
    "\n",
    "#                 outputs = model(data)\n",
    "#                 outputs.to(device)\n",
    "#                 loss = criterion(outputs, vec_target)\n",
    "#                 tmp += loss.item()\n",
    "#                 total += 1\n",
    "\n",
    "#             final_loss = tmp/total\n",
    "#             comptime_end = time.time()\n",
    "#             cli_comptime.append(comptime_end-comptime_start)\n",
    "#             cli_loss.append(final_loss)\n",
    "\n",
    "#     return cli_loss, cli_comptime\n",
    "\n",
    "\n",
    "# def evaluate(model, test_loader, criterion):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Evaluate test accuracy\n",
    "#     \"\"\"\n",
    "\n",
    "#     model.eval()\n",
    "#     loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "#     # Get test accuracy for the current model\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "#             data = data.to(device,non_blocking=True)\n",
    "#             target = target.to(device,non_blocking=True)\n",
    "#             vec_target = vector_encoding(target)\n",
    "\n",
    "            \n",
    "#             vec_target = vec_target.to(device,non_blocking=True)\n",
    "\n",
    "#             # Inference\n",
    "#             outputs = model(data)\n",
    "#             outputs.to(device)\n",
    "#             batch_loss = criterion(outputs, vec_target)\n",
    "#             loss += batch_loss.item()\n",
    "\n",
    "#             # Prediction\n",
    "#             # _, pred_labels = torch.max(outputs, 1)\n",
    "#             pred_labels = get_label(outputs)\n",
    "#             correct += torch.sum(torch.eq(pred_labels, vec_target)).item()/len(pred_labels)\n",
    "#             total += 1\n",
    "\n",
    "#         acc = (correct / total) * 100\n",
    "#         los = loss/total\n",
    "\n",
    "#     return acc, los\n",
    "\n",
    "\n",
    "# def train_text(rank, model, criterion, optimizer, loader, epoch):\n",
    "#     \"\"\"\n",
    "#     train model on the sampled mini-batch for $\\tau$ epochs\n",
    "#     \"\"\"\n",
    "\n",
    "#     model.train()\n",
    "#     loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "#     for batch_idx, (data, target) in enumerate(loader):\n",
    "#         # data loading\n",
    "#         data = data.to(device,non_blocking = True)\n",
    "#         target = target.to(device,non_blocking = True)\n",
    "#         # encode target\n",
    "#         vec_target = vector_encoding(target)\n",
    "\n",
    "#         vec_target = vec_target.to(device,non_blocking = True)\n",
    "        \n",
    "#         outputs = model(data)\n",
    "#         outputs.to(device)\n",
    "#         # print(\"\\n outputs: \", outputs)\n",
    "#         # print(\"\\n vec_target: \", vec_target)\n",
    "#         batch_loss = criterion(outputs, vec_target)\n",
    "\n",
    "#         # backward pass\n",
    "#         batch_loss.backward()\n",
    "\n",
    "#         # gradient clipping\n",
    "#         # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10, norm_type=2)\n",
    "\n",
    "#         # gradient step\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # write log files\n",
    "#         loss += batch_loss.item()\n",
    "\n",
    "#         # Prediction\n",
    "#         # _, pred_labels = torch.max(outputs, 1)\n",
    "#         pred_labels = get_label(outputs)\n",
    "\n",
    "#         correct += torch.sum(torch.eq(pred_labels, vec_target)).item()/len(pred_labels)\n",
    "#         total += 1\n",
    "\n",
    "#         acc = (correct / total)*100\n",
    "#         los = loss / total\n",
    "\n",
    "#         # if batch_idx % args.print_freq == 0 and args.save:\n",
    "#         #     logging.debug('epoch {} itr {}, '\n",
    "#         #                  'rank {}, loss value {:.4f}, train accuracy {:.3f}'\n",
    "#         #                  .format(epoch, batch_idx, rank, los, acc))\n",
    "\n",
    "#         #     with open(args.out_fname, '+a') as f:\n",
    "#         #         print('{ep},{itr},'\n",
    "#         #               '{loss:.4f},-1,-1,'\n",
    "#         #               '{top1:.3f},-1,-1,-1,-1,-1,-1'\n",
    "#         #               .format(ep=epoch, itr=batch_idx,\n",
    "#         #                       loss=los, top1=acc), file=f)\n",
    "\n",
    "#         with open(args.out_fname, '+a') as f:\n",
    "#             print('{ep},{itr},'\n",
    "#                 '{loss:.4f},-1,-1,'\n",
    "#                 '{top1:.3f},-1,-1,-1,-1,-1,-1'\n",
    "#                 .format(ep=epoch, \n",
    "#                         itr=batch_idx,\n",
    "#                         loss=los, \n",
    "#                         top1=acc), file=f)\n",
    "\n",
    "    \n",
    "#     return los, model\n",
    "\n",
    "# def get_label(x):\n",
    "#     \"\"\"\n",
    "#     x : probability of been positive\n",
    "#     return: predicted label\n",
    "#     \"\"\"\n",
    "#     res = torch.zeros_like(x)\n",
    "#     res[x>0.5] = 1\n",
    "    \n",
    "#     return res\n",
    "\n",
    "# def vector_encoding(target):\n",
    "#     \"\"\"\n",
    "#     This is a binary classification\n",
    "\n",
    "#     # 0 for positive, 1 for negative\n",
    "#     \"\"\"\n",
    "#     vector = torch.Tensor([1-i.item() for i in target])\n",
    "#     # vector = torch.Tensor([i.item() for i in target])\n",
    "#     return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91370967-ed33-4957-bd5d-e2abf3a87c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(args.rank, args.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8512563c-5b68-4de5-ad86-518a2351d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin load Glove twitter embedding \n",
      "\n",
      "finish load Glove twitter embedding \n",
      "\n",
      "Randomly select 314 users from  534  candidates\n"
     ]
    }
   ],
   "source": [
    "partition, train_loader, test_loader, dataratios, traindata = util.partition_dataset(314, args, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7106b0fb-6dda-4eed-9dfa-45e8f4fe2ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5102, 0.5108, 0.5121, 0.5099, 0.5106, 0.5098, 0.5109, 0.5103, 0.5107,\n",
       "        0.5114, 0.5105, 0.5108, 0.5104, 0.5114, 0.5102, 0.5109, 0.5090, 0.5099,\n",
       "        0.5100, 0.5100, 0.5109, 0.5114, 0.5099, 0.5103, 0.5115, 0.5094, 0.5110,\n",
       "        0.5095, 0.5117, 0.5105, 0.5092, 0.5105, 0.5087, 0.5102, 0.5109, 0.5114,\n",
       "        0.5096, 0.5110, 0.5106, 0.5100, 0.5131, 0.5095, 0.5097, 0.5108, 0.5113,\n",
       "        0.5101, 0.5107, 0.5107, 0.5104, 0.5099, 0.5107, 0.5108, 0.5103, 0.5099,\n",
       "        0.5111, 0.5099, 0.5099, 0.5090, 0.5092, 0.5093, 0.5092, 0.5118, 0.5105,\n",
       "        0.5105], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.MLP_text(input_size=200, dim_hidden1=128, dim_hidden2=86, dim_hidden3=30, dim_out=1).to(device)\n",
    "\n",
    "data, target = next(iter(train_loader))\n",
    "\n",
    "model(data).shape, data.shape, data[:3,:].shape\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10bfc60f-17b3-49de-905e-3788d34ecefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch save model\n",
    "torch.save(model.state_dict(), \"../models/model1\")\n",
    "\n",
    "# upload mdoel as artifact\n",
    "with mlflow.start_run(run_name=f\"Sentiment_Analysis_seltype_{args.seltype}_powd_{args.powd}_num_users_{args.ensize}_rounds_{args.rounds}\") as run:\n",
    "    mlflow.log_artifact(\"../models/model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ef75438-7d04-4e71-8f3a-2071fbfd9eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98c984bda6e4c6d97595f8644385184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MLP_text(\n",
       "  (layer_input): Linear(in_features=200, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (layer_hidden1): Linear(in_features=128, out_features=86, bias=True)\n",
       "  (layer_hidden2): Linear(in_features=86, out_features=30, bias=True)\n",
       "  (layer_hidden3): Linear(in_features=30, out_features=1, bias=True)\n",
       "  (logsoftmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download model\n",
    "runId = \"2bcbf82e541345cc9e56885cd0de030d\"\n",
    "artifact_name = \"model1\"\n",
    "mlflow.artifacts.download_artifacts(artifact_uri=f\"runs:/{runId}/{artifact_name}\", dst_path=\"../models\")\n",
    "# torch load model\n",
    "model = models.MLP_text(input_size=200, dim_hidden1=128, dim_hidden2=86, dim_hidden3=30, dim_out=1).to(device)\n",
    "model.load_state_dict(torch.load(\"../models/model1\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f54dd881-acd9-4959-85e4-2b79a7a49bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5102, 0.5108, 0.5121, 0.5099, 0.5106, 0.5098, 0.5109, 0.5103, 0.5107,\n",
       "        0.5114, 0.5105, 0.5108, 0.5104, 0.5114, 0.5102, 0.5109, 0.5090, 0.5099,\n",
       "        0.5100, 0.5100, 0.5109, 0.5114, 0.5099, 0.5103, 0.5115, 0.5094, 0.5110,\n",
       "        0.5095, 0.5117, 0.5105, 0.5092, 0.5105, 0.5087, 0.5102, 0.5109, 0.5114,\n",
       "        0.5096, 0.5110, 0.5106, 0.5100, 0.5131, 0.5095, 0.5097, 0.5108, 0.5113,\n",
       "        0.5101, 0.5107, 0.5107, 0.5104, 0.5099, 0.5107, 0.5108, 0.5103, 0.5099,\n",
       "        0.5111, 0.5099, 0.5099, 0.5090, 0.5092, 0.5093, 0.5092, 0.5118, 0.5105,\n",
       "        0.5105], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d813706-3504-4aee-a454-a2109c39c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prediction = model(data)\n",
    "# signature = infer_signature(data, target)\n",
    "\n",
    "# model_info = mlflow.pyfunc.log_model(python_model=model(), \n",
    "#                                      artifact_path=\"my_model\", \n",
    "#                                      signature=data[:3,:].to_numpy()\n",
    "#                                     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
