{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d279705-65f1-44bc-9cdb-04d3ab641dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import time\n",
    "import pathlib\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.backends.cudnn as cudnn\n",
    "\n",
    "from distoptim import fedavg\n",
    "import util_text as util\n",
    "import models\n",
    "# from params import args_parser\n",
    "\n",
    "# define device\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\" # GPU does not speed up training here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72fd96d-b798-44aa-8280-9d800ba1c7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Repository initialized!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/363474a791ea4ebf8b0e375509b6c86a', creation_time=1700855979430, experiment_id='4', last_update_time=1700855979430, lifecycle_stage='active', name='Twitter_Sentiment_Analysis', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "from mlflow.models import infer_signature\n",
    "dagshub.init(repo_owner='peng-ju', repo_name='Power-of-Choice', mlflow=True)\n",
    "mlflow.set_tracking_uri=\"https://dagshub.com/peng-ju/Power-of-Choice.mlflow\"\n",
    "mlflow.set_experiment(experiment_name=\"Twitter_Sentiment_Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea0cce8e-ea26-4fff-b5db-074c2a124e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_parser():\n",
    "    def __init__(self):\n",
    "        self.name = \"20231125\"\n",
    "        self.model = \"MLP\"\n",
    "        self.alpha = 0.2\n",
    "        self.num_classes = 1\n",
    "        self.lr = 0.005\n",
    "        self.momentum = 0\n",
    "        self.bs = 32\n",
    "        self.rounds = 150\n",
    "        self.localE = 100\n",
    "        self.decay = 1 # 1: decay LR, 0: no decay\n",
    "        self.size = 8 # 8, selected clients for updating the model: m\n",
    "        self.powd = 100 # 32\n",
    "        self.fracC = 0.2\n",
    "        self.seltype = \"pow-d\" # pow-d, rand\n",
    "        self.ensize = 314\n",
    "        self.rank = 0\n",
    "        self.rnd_ratio = 0.1\n",
    "        self.delete_ratio = 0.75\n",
    "        self.seed = 1\n",
    "        self.commE = \"store_true\"\n",
    "        self.constant = \"store_true\"\n",
    "        self.dataset = \"twitter\"\n",
    "        self.minimum_tweets = 32 # \n",
    "\n",
    "args = Args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f75e377e-3e40-49b9-a0c7-6266bf0e04ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin load Glove twitter embedding \n",
      "\n",
      "finish load Glove twitter embedding \n",
      "\n",
      "Randomly select 314 users from  2503  candidates\n",
      "\n",
      " dataratios:  [0.003589   0.00202605 0.00347323 0.00544139 0.00266281 0.00237337\n",
      " 0.00191027 0.00671491 0.0031259  0.00208394 0.00266281 0.00879884\n",
      " 0.00243126 0.00248915 0.00248915 0.00196816 0.003589   0.00202605\n",
      " 0.00254703 0.00665702 0.00387844 0.00214182 0.00185239 0.00185239\n",
      " 0.00410999 0.00243126 0.00237337 0.00295224 0.00214182 0.00185239\n",
      " 0.00399421 0.00185239 0.00219971 0.00202605 0.00393632 0.00260492\n",
      " 0.00196816 0.00410999 0.00289436 0.00208394 0.00410999 0.00214182\n",
      " 0.00219971 0.00248915 0.00422576 0.00272069 0.0022576  0.00370478\n",
      " 0.00260492 0.00208394 0.00544139 0.00364689 0.00196816 0.00202605\n",
      " 0.00301013 0.00654124 0.00254703 0.00214182 0.00185239 0.00243126\n",
      " 0.00509407 0.00266281 0.00492041 0.00191027 0.00219971 0.00231548\n",
      " 0.00295224 0.00335745 0.01094067 0.00231548 0.00214182 0.00277858\n",
      " 0.00370478 0.00202605 0.00237337 0.00208394 0.00185239 0.00295224\n",
      " 0.00266281 0.00214182 0.00712012 0.00214182 0.0098987  0.00208394\n",
      " 0.00277858 0.00277858 0.00191027 0.0022576  0.00208394 0.00353111\n",
      " 0.00700434 0.0081042  0.00191027 0.00266281 0.00370478 0.00468886\n",
      " 0.00318379 0.00410999 0.00295224 0.00364689 0.00272069 0.00219971\n",
      " 0.00237337 0.00191027 0.00561505 0.00248915 0.00196816 0.00266281\n",
      " 0.00208394 0.00341534 0.00306802 0.00231548 0.00191027 0.00202605\n",
      " 0.00277858 0.00231548 0.00416787 0.00364689 0.0045152  0.00306802\n",
      " 0.00329957 0.00214182 0.00202605 0.00272069 0.00196816 0.00248915\n",
      " 0.00283647 0.00428365 0.00295224 0.00301013 0.0022576  0.00410999\n",
      " 0.01314038 0.00463097 0.00520984 0.00191027 0.00231548 0.00208394\n",
      " 0.00301013 0.00416787 0.0022576  0.00208394 0.00202605 0.00272069\n",
      " 0.00445731 0.00277858 0.00492041 0.00196816 0.01366136 0.00260492\n",
      " 0.00243126 0.00208394 0.00607815 0.00231548 0.00254703 0.00202605\n",
      " 0.00335745 0.00185239 0.003589   0.00272069 0.00185239 0.00214182\n",
      " 0.00202605 0.00410999 0.00335745 0.00295224 0.00191027 0.00191027\n",
      " 0.0022576  0.00191027 0.00399421 0.00202605 0.00191027 0.00237337\n",
      " 0.00208394 0.00237337 0.00758321 0.00202605 0.00191027 0.00231548\n",
      " 0.00277858 0.00347323 0.00196816 0.00196816 0.0053835  0.00399421\n",
      " 0.00208394 0.00237337 0.00347323 0.00364689 0.00185239 0.00353111\n",
      " 0.00219971 0.00231548 0.00185239 0.00214182 0.00324168 0.00214182\n",
      " 0.00353111 0.00219971 0.00758321 0.00272069 0.00439942 0.00185239\n",
      " 0.00266281 0.00254703 0.00196816 0.00231548 0.00214182 0.00254703\n",
      " 0.01036179 0.00191027 0.00492041 0.00248915 0.00237337 0.00329957\n",
      " 0.00422576 0.00544139 0.00254703 0.00243126 0.00191027 0.00428365\n",
      " 0.00254703 0.00214182 0.00480463 0.00318379 0.00515195 0.00335745\n",
      " 0.0022576  0.00196816 0.00191027 0.00347323 0.00243126 0.00237337\n",
      " 0.00219971 0.00445731 0.00277858 0.00196816 0.00219971 0.00185239\n",
      " 0.00219971 0.0040521  0.00202605 0.00202605 0.00219971 0.00185239\n",
      " 0.0045152  0.00850941 0.00219971 0.00526773 0.00376266 0.00243126\n",
      " 0.00208394 0.00416787 0.00202605 0.00272069 0.00237337 0.00208394\n",
      " 0.00214182 0.00318379 0.00196816 0.00301013 0.01794501 0.00202605\n",
      " 0.00260492 0.00237337 0.00347323 0.00248915 0.00202605 0.00347323\n",
      " 0.00492041 0.00214182 0.00185239 0.00428365 0.003589   0.0022576\n",
      " 0.00266281 0.00185239 0.00602026 0.00324168 0.00196816 0.00196816\n",
      " 0.00185239 0.00387844 0.00260492 0.00196816 0.00295224 0.00219971\n",
      " 0.00191027 0.0031259  0.00231548 0.00387844 0.00191027 0.00208394\n",
      " 0.00301013 0.00619392 0.00185239 0.00191027 0.00185239 0.00387844\n",
      " 0.00324168 0.00272069 0.00781476 0.00515195 0.00480463 0.00219971\n",
      " 0.00208394 0.00237337 0.0053835  0.00382055 0.00289436 0.00422576\n",
      " 0.00243126 0.00364689]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "partition, train_loader, test_loader, dataratios, traindata = util.partition_dataset(args.size, args, 0)\n",
    "print(\"\\n dataratios: \", dataratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50b71d44-6174-489e-a158-1b6453e00fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s - %(message)s', level=logging.INFO)\n",
    "logging.debug('This message should appear on the console')\n",
    "\n",
    "def run(rank, size):\n",
    "    print(\"run start \\n\")\n",
    "    # initiate experiments folder\n",
    "    save_path = './logs/'\n",
    "    fold = 'lr{:.4f}_bs{}_cp{}_a{:.2f}_e{}_r0_n{}_f{:.2f}/'.format(args.lr, \n",
    "                                                                   args.bs, \n",
    "                                                                   args.localE, \n",
    "                                                                   args.alpha, \n",
    "                                                                   args.seed,\n",
    "                                                                   args.ensize, \n",
    "                                                                   args.fracC)\n",
    "    if args.commE:\n",
    "        fold = 'com_'+fold\n",
    "    folder_name = save_path+args.name+'/'+fold\n",
    "    file_name = '{}_rr{:.2f}_dr{:.2f}_lr{:.3f}_bs{:d}_cp{:d}_a{:.2f}_e{}_r{}_n{}_f{:.2f}_p{}.csv'.format(args.seltype,\n",
    "                                                                                                         args.rnd_ratio, \n",
    "                                                                                                         args.delete_ratio, \n",
    "                                                                                                         args.lr, \n",
    "                                                                                                         args.bs, \n",
    "                                                                                                         args.localE,\n",
    "                                                                                                         args.alpha, \n",
    "                                                                                                         args.seed, \n",
    "                                                                                                         rank, \n",
    "                                                                                                         args.ensize, \n",
    "                                                                                                         args.fracC, \n",
    "                                                                                                         args.powd)\n",
    "                                                    \n",
    "    pathlib.Path(folder_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # initiate log files\n",
    "    saveFileName = folder_name + file_name\n",
    "    args.out_fname = saveFileName\n",
    "    with open(args.out_fname, 'w+') as f:\n",
    "        print('Epoch,itr,loss,trainloss,avg:Loss,Prec@1,avg:Prec@1,val,trainval,updtime,comptime,seltime,entime,testacc,testloss', file=f)\n",
    "\n",
    "    # seed for reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Sentiment_Analysis_seltype_{args.seltype}_powd_{args.powd}_num_users_{args.ensize}_rounds_{args.rounds}\") as run:\n",
    "        # MLflow\n",
    "        params = {}\n",
    "        params[\"Client Selection Algorithm\"] = args.seltype\n",
    "        params[\"Total number of clients\"] = args.ensize\n",
    "        params[\"Learning Rate\"] = args.lr\n",
    "        params[\"Batch Size\"] = args.bs\n",
    "        params[\"Total Client Number\"] = args.ensize\n",
    "        params[\"Selected client number m\"] = args.size\n",
    "        params[\"Local iteration number\"] = args.localE\n",
    "        params[\"Powd\"] = args.powd\n",
    "        params[\"Minimum tweets\"] = args.minimum_tweets\n",
    "\n",
    "        # load data\n",
    "        # partition, train_loader, test_loader, dataratios, traindata = util.partition_dataset(size, args, 0)\n",
    "        # print(\"\\n dataratios: \", dataratios)\n",
    "\n",
    "        # tracking client loss values, frequency for each client\n",
    "        # args.ensize -- number of clients\n",
    "        client_freq, client_loss_proxy = np.zeros(args.ensize), np.zeros(args.ensize)\n",
    "\n",
    "        # initialization for client selection\n",
    "        cli_loss, cli_freq, cli_val = np.zeros(args.ensize)+1, np.zeros(args.ensize), np.zeros(args.ensize)\n",
    "\n",
    "        # select client for the 1st round\n",
    "        replace_param = False\n",
    "        if args.seltype =='rand':\n",
    "            replace_param = True\n",
    "\n",
    "        # user id being selected\n",
    "        sel_idx = np.random.choice(args.ensize, size=args.size, replace=replace_param)\n",
    "\n",
    "        # define multilayer perceptron neural network model for sentiment analysis\n",
    "        model = models.MLP_text(input_size=200, dim_hidden1=128, dim_hidden2=86, dim_hidden3=30, dim_out=1).to(device)\n",
    "        \n",
    "        # allocate buffer for global and aggregate parameters\n",
    "        # ref: https://discuss.pytorch.org/t/how-to-assign-an-arbitrary-tensor-to-models-parameter/44082/3\n",
    "        global_parameters = []\n",
    "        aggregate_parameters = []\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                global_parameters.append(param.detach().clone())\n",
    "                aggregate_parameters.append(torch.zeros_like(param)) \n",
    "        \n",
    "        # criterion\n",
    "        # criterion = nn.NLLLoss().to(device)  # for multi-class classifier.\n",
    "        criterion =nn.BCELoss().to(device)  # for binary classifier\n",
    "        params[\"Criterion\"] = \"nn.BCELoss()\"\n",
    "\n",
    "        # select optimizer according to algorithm\n",
    "        optimizer = torch.optim.SGD(model.parameters(), \n",
    "                                    lr=args.lr, \n",
    "                                    momentum=args.momentum, \n",
    "                                    nesterov=False,\n",
    "                                    weight_decay=1e-4)\n",
    "        params[\"Optimizer\"] = \"torch.optim.SGD\"\n",
    "\n",
    "        test_loss_rnd = []\n",
    "        test_accu_rnd = []\n",
    "        train_loss_rnd = []\n",
    "        train_accu_rnd = []\n",
    "\n",
    "\n",
    "        # start communication rounds\n",
    "        for rnd in range(args.rounds):\n",
    "            round_start = time.time()\n",
    "\n",
    "            # (optional) decay learning rate according to round index\n",
    "            if args.decay == True:\n",
    "                # update_learning_rate(optimizer, rnd, args.lr)\n",
    "                if rnd == 50:\n",
    "                    lr = args.lr/2\n",
    "                    logging.info(\"Updating learning rate to {}\".format(lr))\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group[\"lr\"] = lr\n",
    "\n",
    "                if rnd == 100:\n",
    "                    lr = args.lr/4\n",
    "                    logging.info(\"Updating learning rate to {}\".format(lr))\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group[\"lr\"] = lr\n",
    "\n",
    "            # zero aggregate parameters for accumulation of local parameters\n",
    "            with torch.no_grad():\n",
    "                for param in aggregate_parameters:\n",
    "                    param.zero_()\n",
    "\n",
    "            # for each client `i`\n",
    "            for i in sel_idx:\n",
    "                # send global parameters to client `i`\n",
    "                with torch.no_grad():\n",
    "                    for param, global_param in zip(model.parameters(), global_parameters):\n",
    "                        param.copy_(global_param)\n",
    "                \n",
    "                # run E steps of SGD on client `i`\n",
    "                loss_final = 0\n",
    "                comm_update_start = time.time()\n",
    "                for t in range(args.localE):\n",
    "                    singlebatch_loader = util.partitiondata_loader(partition, i, args.bs, traindata)\n",
    "                    loss, model = train_text(rank, model, criterion, optimizer, singlebatch_loader, t)\n",
    "                    loss_final += loss/args.localE #average over localE iterations\n",
    "                comm_update_end = time.time()\n",
    "                update_time = comm_update_end - comm_update_start\n",
    "\n",
    "                # send local parameters from client `i` to server for aggregation\n",
    "                with torch.no_grad():\n",
    "                    weight = 1/args.size\n",
    "                    for aggregate_param, param in zip(aggregate_parameters, model.parameters()):\n",
    "                        aggregate_param.add_(param, alpha=weight)\n",
    "                \n",
    "                # update client frequency and loss values\n",
    "                client_freq[i] += 1\n",
    "                client_loss_proxy[i] = loss_final\n",
    "\n",
    "            not_visited = np.where(client_freq == 0)[0]\n",
    "            for j in not_visited:\n",
    "                if args.seltype == \"afl\":\n",
    "                    client_loss_proxy[j] = -np.inf\n",
    "                else:\n",
    "                    client_loss_proxy[j] = np.inf\n",
    "\n",
    "            # update global parameters\n",
    "            with torch.no_grad():\n",
    "                for global_param, aggregate_param in zip(global_parameters, aggregate_parameters):\n",
    "                    global_param.copy_(aggregate_param)\n",
    "\n",
    "            # set model with global parameters\n",
    "            with torch.no_grad():\n",
    "                for param, global_param in zip(model.parameters(), global_parameters):\n",
    "                    param.copy_(global_param)\n",
    "\n",
    "            # evaluate test accuracy\n",
    "            test_acc, test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "            # evaluate loss values and sync selected frequency\n",
    "            cli_loss, cli_comptime = evaluate_client(model, criterion, partition, traindata)\n",
    "            train_loss = sum([cli_loss[i]*dataratios[i] for i in range(args.ensize)])\n",
    "            # train_loss1 = sum(cli_loss)/args.ensize\n",
    "\n",
    "            # select clients for the next round\n",
    "            sel_time, comp_time = 0, 0\n",
    "            sel_time_start = time.time()\n",
    "            \"\"\"\n",
    "            noteL cli_val, rnd are not useful?\n",
    "            \"\"\"\n",
    "            sel_idx, rnd_idx = util.sel_client(dataratios, cli_loss, cli_val, args, rnd)\n",
    "            # print(f\"len rnd_idx {len(rnd_idx)} idxs_users {len(idxs_users)}\")\n",
    "            sel_time_end = time.time()\n",
    "            sel_time = sel_time_end - sel_time_start\n",
    "\n",
    "            if args.seltype == \"pow-d\" or args.seltype == \"pow-dint\":\n",
    "                comp_time = max([cli_comptime[int(i)] for i in rnd_idx])\n",
    "\n",
    "            # record metrics\n",
    "            round_end = time.time()\n",
    "            round_duration = round(round_end - round_start, 1)\n",
    "            logging.info(f\"[{round_duration} s] Round {rnd} rank {rank} test accuracy {test_acc:.3f} test loss {test_loss:.3f} train loss {train_loss:.3f}\")\n",
    "            # MLflow\n",
    "            mlflow.log_metric(key=\"train_loss\", value=train_loss, step=rnd)\n",
    "            # mlflow.log_metric(key=\"train_accurarcy\", value=train_acc, step=rnd)\n",
    "            mlflow.log_metric(key=\"test_loss\", value=test_loss, step=rnd)\n",
    "            mlflow.log_metric(key=\"test_accurarcy\", value=test_acc, step=rnd)\n",
    "\n",
    "            test_loss_rnd.append(test_loss)\n",
    "            test_accu_rnd.append(test_acc)\n",
    "            train_loss_rnd.append(train_loss)\n",
    "            # train_accu_rnd.append(train_acc)\n",
    "            \n",
    "            # itr = -1 for overal result\n",
    "            with open(args.out_fname, '+a') as f:\n",
    "                print('{ep},{itr},{loss:.4f},{trainloss:.4f},{filler},'\n",
    "                    '{filler},{filler},'\n",
    "                    '{val:.4f},{other:.4f},{updtime:.4f},{comptime:.4f},{seltime:.4f},{entime:.4f}, {testacc:.4f}, {testloss:.4f}'\n",
    "                    .format(ep=rnd, itr=-1, loss=test_loss, trainloss=train_loss,\n",
    "                            filler=-1, val=test_acc, other=train_loss, updtime=update_time, comptime=comp_time,\n",
    "                            seltime=sel_time, entime=update_time+comp_time+sel_time, testacc=test_acc, testloss=test_loss), file=f)\n",
    "        \n",
    "        # MLflow\n",
    "        print(params)\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # save model state_dict and upload as artifact\n",
    "        torch.save(model.state_dict(), \"../models/MLP_Senti.pt\")  # torch save model\n",
    "        \n",
    "        # upload mdoel as artifact\n",
    "        mlflow.log_artifact(\"../models/MLP_Senti.pt\")\n",
    "        \n",
    "        # train_loader = torch.utils.data.DataLoader(traindata, batch_size=args.bs, shuffle=False,\n",
    "        #                                             pin_memory=True)\n",
    "        # data, target = next(iter(train_loader))\n",
    "        # signature = infer_signature(data, model(data))\n",
    "        # model_info = mlflow.pyfunc.log_model(python_model=model, \n",
    "        #                                      artifact_path=\"my_model\", \n",
    "        #                                      signature=signature\n",
    "        #                                     )\n",
    "\n",
    "\n",
    "def evaluate_client(model, criterion, partition, traindata):\n",
    "\n",
    "    '''\n",
    "    Evaluating each client's local loss values for the current global model for client selection\n",
    "    :param model: current global model\n",
    "    :param criterion: loss function\n",
    "    :param partition: dataset dict for clients\n",
    "    :return: cli_loss = list of local loss values, cli_comptime = list of computation time\n",
    "    '''\n",
    "\n",
    "    cli_comptime, cli_loss = [], []\n",
    "    model.eval()\n",
    "\n",
    "    # Get data from client to evaluate local loss on\n",
    "    for i in range(args.ensize):\n",
    "        partitioned = partition[i]\n",
    "\n",
    "        # cpow-d\n",
    "        if args.commE:\n",
    "            seldata_idx = random.sample(range(len(partitioned)), k=int(min(args.bs, len(partitioned))))\n",
    "        else:\n",
    "            seldata_idx = partitioned\n",
    "\n",
    "        other = torch.utils.data.Subset(traindata, indices=seldata_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(other, batch_size=args.bs, shuffle=False,\n",
    "                                                    pin_memory=True)\n",
    "        \n",
    "        # Compute local loss values or proxies for the clients\n",
    "        tmp, total = 0,0\n",
    "        with torch.no_grad():\n",
    "            comptime_start = time.time()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device,non_blocking=True)\n",
    "                target = target.to(device,non_blocking=True)\n",
    "                vec_target = vector_encoding(target)\n",
    "\n",
    "                vec_target = vec_target.to(device,non_blocking=True)\n",
    "\n",
    "                outputs = model(data)\n",
    "                outputs.to(device)\n",
    "                loss = criterion(outputs, vec_target)\n",
    "                tmp += loss.item()\n",
    "                total += 1\n",
    "\n",
    "            final_loss = tmp/total\n",
    "            comptime_end = time.time()\n",
    "            cli_comptime.append(comptime_end-comptime_start)\n",
    "            cli_loss.append(final_loss)\n",
    "\n",
    "    return cli_loss, cli_comptime\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluate test accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    # Get test accuracy for the current model\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            data = data.to(device,non_blocking=True)\n",
    "            target = target.to(device,non_blocking=True)\n",
    "            vec_target = vector_encoding(target)\n",
    "\n",
    "            \n",
    "            vec_target = vec_target.to(device,non_blocking=True)\n",
    "\n",
    "            # Inference\n",
    "            outputs = model(data)\n",
    "            outputs.to(device)\n",
    "            batch_loss = criterion(outputs, vec_target)\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "            # Prediction\n",
    "            # _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = get_label(outputs)\n",
    "            correct += torch.sum(torch.eq(pred_labels, vec_target)).item()/len(pred_labels)\n",
    "            total += 1\n",
    "\n",
    "        acc = (correct / total) * 100\n",
    "        los = loss/total\n",
    "\n",
    "    return acc, los\n",
    "\n",
    "\n",
    "def train_text(rank, model, criterion, optimizer, loader, epoch):\n",
    "    \"\"\"\n",
    "    train model on the sampled mini-batch for $\\tau$ epochs\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        # data loading\n",
    "        data = data.to(device,non_blocking = True)\n",
    "        target = target.to(device,non_blocking = True)\n",
    "        # encode target\n",
    "        vec_target = vector_encoding(target)\n",
    "\n",
    "        vec_target = vec_target.to(device,non_blocking = True)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        outputs.to(device)\n",
    "        # print(\"\\n outputs: \", outputs)\n",
    "        # print(\"\\n vec_target: \", vec_target)\n",
    "        batch_loss = criterion(outputs, vec_target)\n",
    "\n",
    "        # backward pass\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10, norm_type=2)\n",
    "\n",
    "        # gradient step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # write log files\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        # _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = get_label(outputs)\n",
    "\n",
    "        correct += torch.sum(torch.eq(pred_labels, vec_target)).item()/len(pred_labels)\n",
    "        total += 1\n",
    "\n",
    "        acc = (correct / total)*100\n",
    "        los = loss / total\n",
    "\n",
    "        # if batch_idx % args.print_freq == 0 and args.save:\n",
    "        #     logging.debug('epoch {} itr {}, '\n",
    "        #                  'rank {}, loss value {:.4f}, train accuracy {:.3f}'\n",
    "        #                  .format(epoch, batch_idx, rank, los, acc))\n",
    "\n",
    "        #     with open(args.out_fname, '+a') as f:\n",
    "        #         print('{ep},{itr},'\n",
    "        #               '{loss:.4f},-1,-1,'\n",
    "        #               '{top1:.3f},-1,-1,-1,-1,-1,-1'\n",
    "        #               .format(ep=epoch, itr=batch_idx,\n",
    "        #                       loss=los, top1=acc), file=f)\n",
    "\n",
    "        with open(args.out_fname, '+a') as f:\n",
    "            print('{ep},{itr},'\n",
    "                '{loss:.4f},-1,-1,'\n",
    "                '{top1:.3f},-1,-1,-1,-1,-1,-1'\n",
    "                .format(ep=epoch, \n",
    "                        itr=batch_idx,\n",
    "                        loss=los, \n",
    "                        top1=acc), file=f)\n",
    "\n",
    "    \n",
    "    return los, model\n",
    "\n",
    "def get_label(x):\n",
    "    \"\"\"\n",
    "    x : probability of been positive\n",
    "    return: predicted label\n",
    "    \"\"\"\n",
    "    res = torch.zeros_like(x)\n",
    "    res[x>0.5] = 1\n",
    "    \n",
    "    return res\n",
    "\n",
    "def vector_encoding(target):\n",
    "    \"\"\"\n",
    "    This is a binary classification\n",
    "\n",
    "    # 0 for positive, 1 for negative\n",
    "    \"\"\"\n",
    "    vector = torch.Tensor([1-i.item() for i in target])\n",
    "    # vector = torch.Tensor([i.item() for i in target])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91370967-ed33-4957-bd5d-e2abf3a87c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run start \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - [0.9 s] Round 0 rank 0 test accuracy 53.988 test loss 0.693 train loss 0.693\n",
      "INFO - [0.9 s] Round 1 rank 0 test accuracy 51.138 test loss 0.693 train loss 0.692\n",
      "INFO - [1.0 s] Round 2 rank 0 test accuracy 50.699 test loss 0.693 train loss 0.692\n",
      "INFO - [1.0 s] Round 3 rank 0 test accuracy 50.699 test loss 0.693 train loss 0.691\n",
      "INFO - [1.0 s] Round 4 rank 0 test accuracy 50.699 test loss 0.693 train loss 0.691\n",
      "INFO - [1.0 s] Round 5 rank 0 test accuracy 50.699 test loss 0.693 train loss 0.691\n",
      "INFO - [1.3 s] Round 6 rank 0 test accuracy 50.699 test loss 0.693 train loss 0.691\n",
      "INFO - [1.1 s] Round 7 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.690\n",
      "INFO - [1.2 s] Round 8 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.690\n",
      "INFO - [1.0 s] Round 9 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.690\n",
      "INFO - [1.0 s] Round 10 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.690\n",
      "INFO - [1.0 s] Round 11 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.690\n",
      "INFO - [1.1 s] Round 12 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.690\n",
      "INFO - [1.4 s] Round 13 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.689\n",
      "INFO - [1.0 s] Round 14 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.689\n",
      "INFO - [1.0 s] Round 15 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.689\n",
      "INFO - [1.0 s] Round 16 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.689\n",
      "INFO - [1.0 s] Round 17 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.689\n",
      "INFO - [1.0 s] Round 18 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.688\n",
      "INFO - [1.0 s] Round 19 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.689\n",
      "INFO - [1.1 s] Round 20 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.688\n",
      "INFO - [1.0 s] Round 21 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.688\n",
      "INFO - [0.9 s] Round 22 rank 0 test accuracy 50.699 test loss 0.692 train loss 0.688\n",
      "INFO - [0.9 s] Round 23 rank 0 test accuracy 50.699 test loss 0.691 train loss 0.688\n",
      "INFO - [1.2 s] Round 24 rank 0 test accuracy 50.699 test loss 0.691 train loss 0.688\n",
      "INFO - [1.1 s] Round 25 rank 0 test accuracy 50.699 test loss 0.691 train loss 0.687\n",
      "INFO - [1.3 s] Round 26 rank 0 test accuracy 50.699 test loss 0.691 train loss 0.687\n",
      "INFO - [1.3 s] Round 27 rank 0 test accuracy 50.699 test loss 0.691 train loss 0.687\n",
      "INFO - [1.0 s] Round 28 rank 0 test accuracy 50.699 test loss 0.691 train loss 0.687\n",
      "INFO - [1.0 s] Round 29 rank 0 test accuracy 50.699 test loss 0.691 train loss 0.686\n",
      "INFO - [1.0 s] Round 30 rank 0 test accuracy 51.138 test loss 0.690 train loss 0.687\n",
      "INFO - [1.1 s] Round 31 rank 0 test accuracy 51.138 test loss 0.690 train loss 0.686\n",
      "INFO - [1.2 s] Round 32 rank 0 test accuracy 50.699 test loss 0.690 train loss 0.686\n",
      "INFO - [1.1 s] Round 33 rank 0 test accuracy 51.398 test loss 0.690 train loss 0.685\n",
      "INFO - [1.0 s] Round 34 rank 0 test accuracy 50.699 test loss 0.690 train loss 0.684\n",
      "INFO - [1.0 s] Round 35 rank 0 test accuracy 50.699 test loss 0.690 train loss 0.684\n",
      "INFO - [1.1 s] Round 36 rank 0 test accuracy 51.138 test loss 0.689 train loss 0.684\n",
      "INFO - [1.2 s] Round 37 rank 0 test accuracy 52.262 test loss 0.689 train loss 0.684\n",
      "INFO - [1.6 s] Round 38 rank 0 test accuracy 52.262 test loss 0.689 train loss 0.683\n",
      "INFO - [1.3 s] Round 39 rank 0 test accuracy 53.564 test loss 0.689 train loss 0.683\n",
      "INFO - [1.1 s] Round 40 rank 0 test accuracy 53.043 test loss 0.688 train loss 0.682\n",
      "INFO - [0.9 s] Round 41 rank 0 test accuracy 52.262 test loss 0.688 train loss 0.681\n",
      "INFO - [1.0 s] Round 42 rank 0 test accuracy 52.440 test loss 0.688 train loss 0.680\n",
      "INFO - [1.0 s] Round 43 rank 0 test accuracy 54.084 test loss 0.687 train loss 0.679\n",
      "INFO - [1.0 s] Round 44 rank 0 test accuracy 54.084 test loss 0.687 train loss 0.678\n",
      "INFO - [1.0 s] Round 45 rank 0 test accuracy 55.825 test loss 0.686 train loss 0.678\n",
      "INFO - [1.1 s] Round 46 rank 0 test accuracy 59.128 test loss 0.686 train loss 0.677\n",
      "INFO - [1.2 s] Round 47 rank 0 test accuracy 56.168 test loss 0.685 train loss 0.676\n",
      "INFO - [1.0 s] Round 48 rank 0 test accuracy 59.649 test loss 0.685 train loss 0.675\n",
      "INFO - [1.0 s] Round 49 rank 0 test accuracy 60.348 test loss 0.684 train loss 0.674\n",
      "INFO - Updating learning rate to 0.0025\n",
      "INFO - [1.0 s] Round 50 rank 0 test accuracy 58.690 test loss 0.684 train loss 0.672\n",
      "INFO - [1.3 s] Round 51 rank 0 test accuracy 60.088 test loss 0.683 train loss 0.672\n",
      "INFO - [1.3 s] Round 52 rank 0 test accuracy 59.471 test loss 0.683 train loss 0.671\n",
      "INFO - [1.4 s] Round 53 rank 0 test accuracy 59.649 test loss 0.682 train loss 0.671\n",
      "INFO - [1.0 s] Round 54 rank 0 test accuracy 59.471 test loss 0.682 train loss 0.670\n",
      "INFO - [1.0 s] Round 55 rank 0 test accuracy 59.910 test loss 0.682 train loss 0.668\n",
      "INFO - [1.3 s] Round 56 rank 0 test accuracy 60.430 test loss 0.681 train loss 0.668\n",
      "INFO - [1.1 s] Round 57 rank 0 test accuracy 60.513 test loss 0.681 train loss 0.666\n",
      "INFO - [1.5 s] Round 58 rank 0 test accuracy 60.773 test loss 0.680 train loss 0.666\n",
      "INFO - [1.0 s] Round 59 rank 0 test accuracy 62.418 test loss 0.680 train loss 0.665\n",
      "INFO - [1.0 s] Round 60 rank 0 test accuracy 62.939 test loss 0.679 train loss 0.663\n",
      "INFO - [1.0 s] Round 61 rank 0 test accuracy 63.199 test loss 0.678 train loss 0.662\n",
      "INFO - [1.0 s] Round 62 rank 0 test accuracy 62.075 test loss 0.678 train loss 0.661\n",
      "INFO - [1.0 s] Round 63 rank 0 test accuracy 62.678 test loss 0.677 train loss 0.660\n",
      "INFO - [0.9 s] Round 64 rank 0 test accuracy 62.939 test loss 0.676 train loss 0.659\n",
      "INFO - [1.0 s] Round 65 rank 0 test accuracy 63.555 test loss 0.675 train loss 0.657\n",
      "INFO - [1.0 s] Round 66 rank 0 test accuracy 63.816 test loss 0.675 train loss 0.656\n",
      "INFO - [1.1 s] Round 67 rank 0 test accuracy 63.555 test loss 0.674 train loss 0.654\n",
      "INFO - [1.0 s] Round 68 rank 0 test accuracy 63.555 test loss 0.673 train loss 0.653\n",
      "INFO - [1.0 s] Round 69 rank 0 test accuracy 63.555 test loss 0.672 train loss 0.650\n",
      "INFO - [1.0 s] Round 70 rank 0 test accuracy 63.816 test loss 0.671 train loss 0.648\n",
      "INFO - [1.0 s] Round 71 rank 0 test accuracy 63.555 test loss 0.670 train loss 0.646\n",
      "INFO - [1.0 s] Round 72 rank 0 test accuracy 64.076 test loss 0.669 train loss 0.644\n",
      "INFO - [0.9 s] Round 73 rank 0 test accuracy 64.337 test loss 0.668 train loss 0.642\n",
      "INFO - [1.0 s] Round 74 rank 0 test accuracy 64.337 test loss 0.667 train loss 0.640\n",
      "INFO - [1.0 s] Round 75 rank 0 test accuracy 63.816 test loss 0.666 train loss 0.635\n",
      "INFO - [1.0 s] Round 76 rank 0 test accuracy 64.337 test loss 0.665 train loss 0.635\n",
      "INFO - [0.9 s] Round 77 rank 0 test accuracy 65.556 test loss 0.663 train loss 0.631\n",
      "INFO - [0.9 s] Round 78 rank 0 test accuracy 64.337 test loss 0.662 train loss 0.626\n",
      "INFO - [0.9 s] Round 79 rank 0 test accuracy 64.775 test loss 0.660 train loss 0.624\n",
      "INFO - [0.9 s] Round 80 rank 0 test accuracy 65.036 test loss 0.659 train loss 0.624\n",
      "INFO - [1.0 s] Round 81 rank 0 test accuracy 65.296 test loss 0.657 train loss 0.618\n",
      "INFO - [0.9 s] Round 82 rank 0 test accuracy 65.556 test loss 0.656 train loss 0.616\n",
      "INFO - [1.0 s] Round 83 rank 0 test accuracy 65.036 test loss 0.654 train loss 0.611\n",
      "INFO - [1.2 s] Round 84 rank 0 test accuracy 65.036 test loss 0.653 train loss 0.608\n",
      "INFO - [1.3 s] Round 85 rank 0 test accuracy 65.036 test loss 0.651 train loss 0.602\n",
      "INFO - [1.0 s] Round 86 rank 0 test accuracy 64.775 test loss 0.650 train loss 0.597\n",
      "INFO - [0.9 s] Round 87 rank 0 test accuracy 65.036 test loss 0.648 train loss 0.597\n",
      "INFO - [0.9 s] Round 88 rank 0 test accuracy 64.775 test loss 0.646 train loss 0.592\n",
      "INFO - [1.0 s] Round 89 rank 0 test accuracy 65.036 test loss 0.644 train loss 0.586\n",
      "INFO - [1.0 s] Round 90 rank 0 test accuracy 65.735 test loss 0.643 train loss 0.579\n",
      "INFO - [1.0 s] Round 91 rank 0 test accuracy 65.556 test loss 0.640 train loss 0.575\n",
      "INFO - [1.0 s] Round 92 rank 0 test accuracy 65.556 test loss 0.638 train loss 0.569\n",
      "INFO - [0.9 s] Round 93 rank 0 test accuracy 65.474 test loss 0.637 train loss 0.566\n",
      "INFO - [1.0 s] Round 94 rank 0 test accuracy 66.338 test loss 0.634 train loss 0.558\n",
      "INFO - [0.9 s] Round 95 rank 0 test accuracy 65.995 test loss 0.633 train loss 0.552\n",
      "INFO - [1.0 s] Round 96 rank 0 test accuracy 66.255 test loss 0.632 train loss 0.547\n",
      "INFO - [0.9 s] Round 97 rank 0 test accuracy 66.516 test loss 0.630 train loss 0.539\n",
      "INFO - [1.0 s] Round 98 rank 0 test accuracy 67.297 test loss 0.628 train loss 0.533\n",
      "INFO - [0.9 s] Round 99 rank 0 test accuracy 66.776 test loss 0.626 train loss 0.530\n",
      "INFO - Updating learning rate to 0.00125\n",
      "INFO - [0.9 s] Round 100 rank 0 test accuracy 67.558 test loss 0.626 train loss 0.527\n",
      "INFO - [1.0 s] Round 101 rank 0 test accuracy 67.297 test loss 0.625 train loss 0.524\n",
      "INFO - [0.9 s] Round 102 rank 0 test accuracy 67.037 test loss 0.623 train loss 0.520\n",
      "INFO - [0.9 s] Round 103 rank 0 test accuracy 67.818 test loss 0.622 train loss 0.516\n",
      "INFO - [1.0 s] Round 104 rank 0 test accuracy 67.558 test loss 0.621 train loss 0.509\n",
      "INFO - [1.0 s] Round 105 rank 0 test accuracy 67.297 test loss 0.622 train loss 0.507\n",
      "INFO - [1.0 s] Round 106 rank 0 test accuracy 67.818 test loss 0.620 train loss 0.507\n",
      "INFO - [0.9 s] Round 107 rank 0 test accuracy 68.078 test loss 0.618 train loss 0.500\n",
      "INFO - [0.9 s] Round 108 rank 0 test accuracy 67.558 test loss 0.618 train loss 0.496\n",
      "INFO - [0.9 s] Round 109 rank 0 test accuracy 67.818 test loss 0.616 train loss 0.497\n",
      "INFO - [0.9 s] Round 110 rank 0 test accuracy 68.078 test loss 0.615 train loss 0.493\n",
      "INFO - [1.0 s] Round 111 rank 0 test accuracy 68.078 test loss 0.614 train loss 0.487\n",
      "INFO - [1.0 s] Round 112 rank 0 test accuracy 67.818 test loss 0.615 train loss 0.483\n",
      "INFO - [0.9 s] Round 113 rank 0 test accuracy 68.339 test loss 0.615 train loss 0.486\n",
      "INFO - [1.0 s] Round 114 rank 0 test accuracy 68.339 test loss 0.611 train loss 0.480\n",
      "INFO - [1.0 s] Round 115 rank 0 test accuracy 68.339 test loss 0.613 train loss 0.473\n",
      "INFO - [0.9 s] Round 116 rank 0 test accuracy 68.599 test loss 0.609 train loss 0.471\n",
      "INFO - [0.9 s] Round 117 rank 0 test accuracy 68.339 test loss 0.610 train loss 0.466\n",
      "INFO - [1.1 s] Round 118 rank 0 test accuracy 68.599 test loss 0.609 train loss 0.463\n",
      "INFO - [1.0 s] Round 119 rank 0 test accuracy 68.078 test loss 0.610 train loss 0.459\n",
      "INFO - [1.0 s] Round 120 rank 0 test accuracy 68.078 test loss 0.609 train loss 0.455\n",
      "INFO - [0.9 s] Round 121 rank 0 test accuracy 68.599 test loss 0.607 train loss 0.455\n",
      "INFO - [0.9 s] Round 122 rank 0 test accuracy 68.599 test loss 0.603 train loss 0.448\n",
      "INFO - [1.0 s] Round 123 rank 0 test accuracy 68.339 test loss 0.604 train loss 0.439\n",
      "INFO - [0.9 s] Round 124 rank 0 test accuracy 68.161 test loss 0.607 train loss 0.443\n",
      "INFO - [1.0 s] Round 125 rank 0 test accuracy 68.599 test loss 0.604 train loss 0.433\n",
      "INFO - [1.0 s] Round 126 rank 0 test accuracy 68.599 test loss 0.601 train loss 0.430\n",
      "INFO - [1.0 s] Round 127 rank 0 test accuracy 68.161 test loss 0.602 train loss 0.430\n",
      "INFO - [1.0 s] Round 128 rank 0 test accuracy 67.900 test loss 0.602 train loss 0.425\n",
      "INFO - [0.9 s] Round 129 rank 0 test accuracy 67.900 test loss 0.599 train loss 0.413\n",
      "INFO - [0.9 s] Round 130 rank 0 test accuracy 69.380 test loss 0.596 train loss 0.418\n",
      "INFO - [1.0 s] Round 131 rank 0 test accuracy 67.462 test loss 0.600 train loss 0.420\n",
      "INFO - [1.0 s] Round 132 rank 0 test accuracy 68.161 test loss 0.600 train loss 0.409\n",
      "INFO - [1.0 s] Round 133 rank 0 test accuracy 68.243 test loss 0.596 train loss 0.407\n",
      "INFO - [1.0 s] Round 134 rank 0 test accuracy 68.243 test loss 0.595 train loss 0.409\n",
      "INFO - [1.0 s] Round 135 rank 0 test accuracy 68.942 test loss 0.590 train loss 0.402\n",
      "INFO - [1.0 s] Round 136 rank 0 test accuracy 68.942 test loss 0.592 train loss 0.394\n",
      "INFO - [1.7 s] Round 137 rank 0 test accuracy 68.681 test loss 0.594 train loss 0.392\n",
      "INFO - [1.1 s] Round 138 rank 0 test accuracy 68.942 test loss 0.590 train loss 0.391\n",
      "INFO - [1.0 s] Round 139 rank 0 test accuracy 69.380 test loss 0.591 train loss 0.392\n",
      "INFO - [0.9 s] Round 140 rank 0 test accuracy 69.202 test loss 0.587 train loss 0.377\n",
      "INFO - [0.9 s] Round 141 rank 0 test accuracy 69.202 test loss 0.590 train loss 0.381\n",
      "INFO - [0.9 s] Round 142 rank 0 test accuracy 69.723 test loss 0.583 train loss 0.376\n",
      "INFO - [1.0 s] Round 143 rank 0 test accuracy 70.244 test loss 0.581 train loss 0.372\n",
      "INFO - [0.9 s] Round 144 rank 0 test accuracy 70.683 test loss 0.582 train loss 0.373\n",
      "INFO - [1.0 s] Round 145 rank 0 test accuracy 70.244 test loss 0.582 train loss 0.370\n",
      "INFO - [1.0 s] Round 146 rank 0 test accuracy 70.504 test loss 0.583 train loss 0.366\n",
      "INFO - [1.0 s] Round 147 rank 0 test accuracy 70.244 test loss 0.580 train loss 0.361\n",
      "INFO - [1.0 s] Round 148 rank 0 test accuracy 70.504 test loss 0.580 train loss 0.358\n",
      "INFO - [1.1 s] Round 149 rank 0 test accuracy 71.464 test loss 0.576 train loss 0.351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Client Selection Algorithm': 'pow-d', 'Total number of clients': 314, 'Learning Rate': 0.005, 'Batch Size': 32, 'Total Client Number': 314, 'Selected client number m': 8, 'Local iteration number': 100, 'Powd': 100, 'Minimum tweets': 32, 'Criterion': 'nn.BCELoss()', 'Optimizer': 'torch.optim.SGD'}\n"
     ]
    }
   ],
   "source": [
    "run(args.rank, args.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512563c-5b68-4de5-ad86-518a2351d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition, train_loader, test_loader, dataratios, traindata = util.partition_dataset(314, args, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106b0fb-6dda-4eed-9dfa-45e8f4fe2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.MLP_text(input_size=200, dim_hidden1=128, dim_hidden2=86, dim_hidden3=30, dim_out=1).to(device)\n",
    "\n",
    "# data, target = next(iter(train_loader))\n",
    "\n",
    "# model(data).shape, data.shape, data[:3,:].shape\n",
    "# model(data), data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10bfc60f-17b3-49de-905e-3788d34ecefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch save model\n",
    "# torch.save(model.state_dict(), \"../models/model1\")\n",
    "\n",
    "# # upload mdoel as artifact\n",
    "with mlflow.start_run(run_name=f\"Sentiment_Analysis_seltype_{args.seltype}_powd_{args.powd}_num_users_{args.ensize}_rounds_{args.rounds}\") as run:\n",
    "    # mlflow.log_artifact(\"../models/model1\")\n",
    "    params = {'Learning Rate': 0.005,\n",
    "              'Batch Size': 32, \n",
    "              'Total Client Number': 314, \n",
    "              'Selected client number m': 8, \n",
    "              'Local iteration number': 100, \n",
    "              'Powd': 32,\n",
    "              'Criterion': 'nn.BCELoss()',\n",
    "              'Optimizer': 'torch.optim.SGD'\n",
    "             }\n",
    "    mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef75438-7d04-4e71-8f3a-2071fbfd9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download model\n",
    "# runId = \"2bcbf82e541345cc9e56885cd0de030d\"\n",
    "# artifact_name = \"model1\"\n",
    "# mlflow.artifacts.download_artifacts(artifact_uri=f\"runs:/{runId}/{artifact_name}\", dst_path=\"../models\")\n",
    "# # torch load model\n",
    "# model = models.MLP_text(input_size=200, dim_hidden1=128, dim_hidden2=86, dim_hidden3=30, dim_out=1).to(device)\n",
    "# model.load_state_dict(torch.load(\"../models/model1\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54dd881-acd9-4959-85e4-2b79a7a49bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d813706-3504-4aee-a454-a2109c39c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prediction = model(data)\n",
    "# signature = infer_signature(data, target)\n",
    "\n",
    "# model_info = mlflow.pyfunc.log_model(python_model=model(), \n",
    "#                                      artifact_path=\"my_model\", \n",
    "#                                      signature=data[:3,:].to_numpy()\n",
    "#                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32cddb-8824-4f4a-bf45-fb0a75047a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
